{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Keyword Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "* Read CMG keywords\n",
    "    * Extract from 2022.10 manuals  \n",
    "    * Search in the \\Data\\Index_Chunk0.js files (0 to 9)\n",
    "        * Regex:\n",
    "\n",
    "        `'([^']+)':\\{l:\\[\\{u:'([^']+)',t:'([^']+')`\n",
    "        \n",
    "        * First group: keyword.\n",
    "            * Add to keyword list.\n",
    "        * Second group: htm file with full description.\n",
    "            * Ignore anything including and after '#' and read file.\n",
    "            * Search all keywords and options inside DEFINITIONS section in the file:\n",
    "\n",
    "            `(<span class=\"keyword\">[^*]*\\*([^<]+\\w)</span>`\n",
    "        \n",
    "            * Ignore any keyword that is found in the short description, add all others as options.\n",
    "        * Third group: short description, with all keywords described in the htm file\n",
    "            * Extract all associated keywords to be used in options search.\n",
    "        \n",
    "            `\\*([0-9a-z-_]+)`\n",
    "\n",
    "* Compile sets with keywords and options.\n",
    "* Write lines in the VsCode language syntax.\n",
    "    * Keywords are divided by first letter to reduce size of line in json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../../../common_folder.txt', 'r', encoding='utf-8') as f:\n",
    "    common_folder = Path(f.readline().strip()) \n",
    "simulators = {'GEM', 'IMEX', 'STARS'}\n",
    "output_keywords = 'keywords.txt'\n",
    "output_options = 'options.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_set_to_text_file(file_path, input_set):\n",
    "    file_path = Path(file_path)\n",
    "    try:\n",
    "        content = '\\n'.join(map(str, input_set))\n",
    "        file_path.write_text(data=content, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to '{file_path}': {e}\")\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    try:\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "        return None\n",
    "    \n",
    "def read_file_into_set(file_path):\n",
    "    content = read_text_file(file_path)\n",
    "    if content is not None:\n",
    "        return content.split('\\n')\n",
    "    else:\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_files(folder_path):\n",
    "    files = []\n",
    "    for file_path in folder_path.rglob('Index_Chunk*.js'):\n",
    "        if file_path.is_file():\n",
    "            files.append(file_path)\n",
    "    return files\n",
    "\n",
    "def parse_additional_options(content):\n",
    "    pattern1 = r'<table[^>]*>(?:(?!<\\/table>)[\\s\\S])*?>(?=.*>condition<\\/span>)(?:(?!<\\/table>)[\\s\\S])*<\\/table>'\n",
    "    matches = re.findall(pattern1, content)\n",
    "\n",
    "    pattern2 = r\"<td>(\\s|\\n)*(<p>)*([-_A-Z0-9]+)(\\s+'?\\w+'?)?(</p>)*(\\s|\\n)*</td>\"\n",
    "    pattern3 = r'<a name=\"(\\w+)\">[^<]*</a>'\n",
    "    out = set()\n",
    "    for m in matches:\n",
    "        for s in re.findall(pattern2, m):\n",
    "            if len(s) > 1:\n",
    "                if s[2].strip() != '':\n",
    "                    out.add(s[2].strip())\n",
    "        for s in re.findall(pattern3, m):\n",
    "            if s.strip() != '':\n",
    "                out.add(s.strip())\n",
    "    \n",
    "    return out\n",
    "\n",
    "def parse_description_file(file_path, keywords):\n",
    "    content = read_text_file(file_path)\n",
    "    \n",
    "    long_description = ''\n",
    "    purpose = re.findall(r'>PURPOSE:?</(h3|p)>(.*?)(?=<(h3)>|$)', content, re.DOTALL)\n",
    "    if len(purpose) > 0:\n",
    "        if len(purpose[0]) > 1:\n",
    "            long_description = re.sub(r'<[^>]*>', '', purpose[0][1])  \n",
    "            long_description = re.sub(r'\\n', '', long_description).strip()\n",
    "            long_description = re.sub(r'\\s+', ' ', long_description)\n",
    "\n",
    "    options = set()\n",
    "    definitions = re.findall(r'>DEFINITIONS:</h3>(.*?)(?=<h3>|$)', content, re.DOTALL)\n",
    "    if len(definitions) > 0:\n",
    "        matches = re.findall(r'<span class=\"keyword\">[^*]*\\*([^<]*\\w)</span>', definitions[0])\n",
    "\n",
    "        for match in matches:\n",
    "            if match not in keywords:\n",
    "                if not match.isdigit():\n",
    "                    options.add(match)\n",
    "\n",
    "        options = options.union(parse_additional_options(definitions[0]))\n",
    "\n",
    "    return long_description, options\n",
    "\n",
    "def parse_index_file(file_path):\n",
    "    content = read_text_file(file_path)\n",
    "    matches = re.findall(r\"'([^']+)':\\{l:\\[\\{u:'([^']+)',t:'([^']+')\", content)\n",
    "\n",
    "    data = dict()\n",
    "    for match in matches:\n",
    "        file = match[1].split('#')[0]\n",
    "        description = match[2].split(' *')[0]\n",
    "        keywords = [x[0] for x in re.findall(r\"\\*((\\w|-)+)\", match[2])]\n",
    "        htm_file = Path(str(file_path.parent.parent) + file)\n",
    "        long_description, options = parse_description_file(htm_file, keywords)\n",
    "        data[match[0]] = {'file': file, \n",
    "                          'description':description, \n",
    "                          'long description': long_description, \n",
    "                          'keywords':keywords, \n",
    "                          'options':options}\n",
    "    return data\n",
    "\n",
    "def read_index(folder):\n",
    "    print(f'Reading {folder.name}')\n",
    "    index_files = find_index_files(folder)\n",
    "    print(f'  Found {len(index_files)} index files')\n",
    "    data = dict()\n",
    "    for index_file in index_files:\n",
    "        data.update(parse_index_file(index_file))\n",
    "    print(f'  {len(data)} keywords found.')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading STARS\n",
      "  Found 9 index files\n",
      "  1253 keywords found.\n",
      "Reading GEM\n",
      "  Found 10 index files\n",
      "  1223 keywords found.\n",
      "Reading IMEX\n",
      "  Found 7 index files\n",
      "  1038 keywords found.\n"
     ]
    }
   ],
   "source": [
    "data = dict()\n",
    "for simulator in simulators:\n",
    "    data[simulator] = read_index(common_folder / simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output to a Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sets_to_lists(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {key: convert_sets_to_lists(value) for key, value in data.items()}\n",
    "    elif isinstance(data, set):\n",
    "        return list(sorted(data))\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_lists = convert_sets_to_lists(data)\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data_with_lists, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL\t1935 keywords\n",
      "STARS\t1253 keywords\n",
      "GEM\t1223 keywords\n",
      "IMEX\t1038 keywords\n",
      "\n",
      "ALL\t2071 options\n",
      "STARS\t1064 options\n",
      "GEM\t1316 options\n",
      "IMEX\t1268 options\n"
     ]
    }
   ],
   "source": [
    "keywords = dict()\n",
    "\n",
    "keywords['ALL'] = set()\n",
    "for simulator in simulators:\n",
    "    keywords[simulator] = set(data[simulator].keys())\n",
    "    keywords['ALL'] = keywords['ALL'].union(keywords[simulator])\n",
    "\n",
    "for simulator,keyword_set in keywords.items():\n",
    "    print(f'{simulator}\\t{len(keyword_set)} keywords')\n",
    "\n",
    "print('')\n",
    "\n",
    "options = dict()\n",
    "\n",
    "options['ALL'] = set()\n",
    "for simulator in simulators:\n",
    "    options[simulator] = set()\n",
    "    for opts in data[simulator].values():\n",
    "        options[simulator] = options[simulator].union(opts['options'])\n",
    "    options['ALL'] = options['ALL'].union(options[simulator])\n",
    "\n",
    "for simulator,options_set in options.items():\n",
    "    print(f'{simulator}\\t{len(options_set)} options')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsets_by_first_char(input_set):\n",
    "    subsets = {}\n",
    "    for item in input_set:\n",
    "        first_char = item[0]\n",
    "        if first_char not in subsets:\n",
    "            subsets[first_char] = set()\n",
    "        subsets[first_char].add(item)\n",
    "    return subsets\n",
    "\n",
    "def output_set(values, file_path, type_name, is_keyword):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for simulator in sorted(set(values.keys())):\n",
    "            file.write(f\"'{simulator}'\\n\")\n",
    "            for sub_keyword_set in subsets_by_first_char(sorted(values[simulator])).values():\n",
    "                file.write('\\t\\t\\t\\t},{\\n')\n",
    "                file.write(f'\\t\\t\\t\\t\\t\"name\": \"{type_name}\",\\n')\t\n",
    "                set_str = '|'.join(map(str, sorted(sub_keyword_set))).replace('+','\\\\\\\\+')\n",
    "                if is_keyword:\n",
    "                    file.write(f'\\t\\t\\t\\t\\t\"match\": \"^\\\\\\\\s*\\\\\\\\*?({set_str})(?=(\\\\\\\\s|$))\"\\n')\n",
    "                else:\n",
    "                    file.write(f'\\t\\t\\t\\t\\t\"match\": \"(?<=(\\\\\\\\s|^))\\\\\\\\*?({set_str})(?=(\\\\\\\\s|$))\"\\n')\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_set(keywords, output_keywords, \"entity.name.function\", True)\n",
    "output_set(options, output_options, \"entity.name.type\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
